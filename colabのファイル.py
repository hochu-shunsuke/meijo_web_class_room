# -*- coding: utf-8 -*-
"""webclassèª²é¡Œå–å¾—.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1quzhlPPpRdZ-RLYrjoJy1s1uqOfr7SRU
"""



# ğŸŸ¥1ç•ªç›®ã®ã‚³ãƒ¼ãƒ‰ã‚»ãƒ«ğŸŸ¥

# ----------------------------------------------------------------------
# ğŸŒŸ STEP 1: åˆæœŸè¨­å®š (ã“ã“ã ã‘å¤‰æ›´) ğŸŒŸ
# ----------------------------------------------------------------------

# 1. å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
# !pip install requests beautifulsoup4

# 2. ğŸš¨ GAS Webã‚¢ãƒ—ãƒªã®URLã‚’ã“ã“ã«è²¼ã‚Šä»˜ã‘ã¦ãã ã•ã„ ğŸš¨
#    (ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã®ã€Œâœ¨ ã‚·ã‚¹ãƒ†ãƒ åˆæœŸè¨­å®šã€ãƒ¡ãƒ‹ãƒ¥ãƒ¼ã‹ã‚‰å–å¾—ã—ãŸã‚‚ã®)
GAS_WEB_APP_URL = 'ã‚ãªãŸã®GAS Webã‚¢ãƒ—ãƒªURLã‚’ã“ã“ã«è²¼ã‚Šä»˜ã‘ã¦ãã ã•ã„'

# 3. webclassã®ãƒ­ã‚°ã‚¤ãƒ³æƒ…å ±ã‚’Colabã®Secretsã«è¨­å®š
# å·¦å´ãƒ¡ãƒ‹ãƒ¥ãƒ¼ã®ï¼Œã‚·ãƒ¼ã‚¯ãƒ¬ãƒƒãƒˆã«ä»¥ä¸‹ã®æƒ…å ±ã‚’è¿½åŠ ã—ã¦ãã ã•ã„
# - ã€ŒUSER_IDã€: ã‚ãªãŸã®WebClassã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ID
# - ã€ŒPASSWORDã€: ã‚ãªãŸã®WebClassã®ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰


# ğŸŸ¥2ç•ªç›®ã®ã‚³ãƒ¼ãƒ‰ã‚»ãƒ«ï¼ˆä¿®æ­£ç¦æ­¢ï¼‰ğŸŸ¥

# ----------------------------------------------------------------------
# ğŸš€ STEP 2: å®Ÿè¡Œï¼ˆã“ã®ã‚»ãƒ«å…¨ä½“ã‚’ã‚¯ãƒªãƒƒã‚¯ï¼ï¼‰ ğŸš€
# ----------------------------------------------------------------------

# ----------------------------------------------------------------------
# âš™ï¸ ã‚·ã‚¹ãƒ†ãƒ ã‚³ã‚¢ãƒ­ã‚¸ãƒƒã‚¯ï¼ˆã“ã“ã‹ã‚‰ä¸‹ã¯å¤‰æ›´ã—ãªã„ã§ãã ã•ã„ï¼‰
# ----------------------------------------------------------------------
import re
import urllib.parse
import json
import requests
import time
import random
import os
from typing import Optional, Dict
from bs4 import BeautifulSoup

# --- WebClass Client å®šæ•° ---
SSO_URL = 'https://slbsso.meijo-u.ac.jp/opensso/json/authenticate'
MAX_RETRIES = 5
RETRY_DELAY = 1
WEBCLASS_BASE_URL = 'https://rpwebcls.meijo-u.ac.jp'
USER_AGENTS = [
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/15.1 Safari/605.1.15',
    'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.114 Safari/537.36',
    'Mozilla/5.0 (Windows NT 6.1; WOW64; rv:40.0) Gecko/20100101 Firefox/40.1',
    'Mozilla/5.0 (iPhone; CPU iPhone OS 14_0 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0 Mobile/15A372 Safari/604.1',
    'Mozilla/5.0 (Linux; Android 10; SM-G975F) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.106 Mobile Safari/537.36',
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/117.0'
]

REDIRECT_REGEX = re.compile(r'window.location.href\s*=\s*"([^"]+)"')
ID_REGEX = re.compile(r'id=([a-f0-9]+)')

# --- WebClass Client/Parser ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•° (å®šç¾©ã¯çœç•¥) ---
# ... (build_headers, _get_sso_token, _get_acs_path, WebClassClient, parse_course_contents ã®å®šç¾©) ...

def build_headers(referer: Optional[str] = None) -> Dict[str, str]:
    ua = random.choice(USER_AGENTS)
    headers = {
        "User-Agent": ua, "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.9",
        "Accept-Language": "ja-JP,ja;q=0.9,en-US;q=0.8,en;q=0.7",
        "Accept-Encoding": "gzip, deflate, br", "Connection": "keep-alive",
    }
    if referer: headers["Referer"] = referer
    return headers

def _get_sso_token(userid, password):
    headers = {'Content-Type': 'application/json'}
    try:
        source_res = requests.post(SSO_URL, headers=headers)
        source_res.raise_for_status()
        jsn = source_res.json()
        jsn["callbacks"][0]["input"][0]["value"] = userid
        jsn["callbacks"][1]["input"][0]["value"] = password

        for atdatat in range(MAX_RETRIES):
            token_res = requests.post(SSO_URL, headers=headers, json=jsn)
            if token_res.status_code == 200:
                token_data = token_res.json()
                print(f"SSOãƒˆãƒ¼ã‚¯ãƒ³å–å¾—æˆåŠŸï¼")
                return token_data["tokenId"]
            print(f"èªè¨¼è©¦è¡Œ {atdatat + 1}/{MAX_RETRIES} å¤±æ•— (Status: {token_res.status_code}). {RETRY_DELAY}ç§’å¾Œãƒªãƒˆãƒ©ã‚¤...")
            time.sleep(RETRY_DELAY)
        raise Exception(f"ãƒˆãƒ¼ã‚¯ãƒ³å–å¾—å¤±æ•— (Status: {token_res.status_code})")
    except requests.exceptions.RequestException as e:
        print(f"SSOã‚µãƒ¼ãƒãƒ¼ã¸ã®æ¥ç¶šã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        raise
    except (KeyError, IndexError, json.JSONDecodeError) as e:
        print(f"SSOã‚µãƒ¼ãƒãƒ¼ã®å¿œç­”å½¢å¼ãŒäºˆæœŸã—ãŸã‚‚ã®ã§ã¯ã‚ã‚Šã¾ã›ã‚“: {e}")
        raise

def _get_acs_path(source):
    soup = BeautifulSoup(source, "html.parser")
    script_tag = soup.find("script")
    if script_tag and script_tag.string and '"' in script_tag.string:
        exccode = script_tag.string
        parts = exccode.split('"')
        if len(parts) > 1: return parts[1].replace('&amp;', "&")
    return None

class WebClassClient:
    def __init__(self, userid, password):
        print("èªè¨¼ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’é–‹å§‹ã—ã¾ã™...")
        self.session = requests.Session()
        self.session.headers.update(build_headers())
        self.base_url = WEBCLASS_BASE_URL
        self.dashboard_url = None
        self._login(userid, password)

    def _login(self, userid, password):
        try:
            token_id = _get_sso_token(userid, password)
            login_url = f"{self.base_url}/webclass/login.php?auth_mode=SAML"
            res = self.session.get(login_url, allow_redirects=False)
            sso_location = res.headers["Location"]
            sso_cookies = {"iPlanetDirectoryPro": token_id}
            sso_res = self.session.get(sso_location, cookies=sso_cookies)
            sso_res.raise_for_status()
            soup = BeautifulSoup(sso_res.text, "html.parser")
            inputs = soup.find_all("input")
            data = {"SAMLResponse": inputs[0].attrs["value"], "RelayState": inputs[1].attrs["value"]}
            acs_url = f"{self.base_url}/simplesaml/module.php/saml/sp/saml2-acs.php/default-sp"
            self.session.post(acs_url, data=data, allow_redirects=False)
            login_php_res = self.session.get(login_url, allow_redirects=False)
            acs_path = _get_acs_path(login_php_res.text)
            if not acs_path: raise Exception("æœ€çµ‚çš„ãªãƒªãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆãƒ‘ã‚¹(acsPath)ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸï¼")
            self.dashboard_url = urllib.parse.urljoin(self.base_url, acs_path)
            self.session.get(self.dashboard_url).raise_for_status()
            print(f"ãƒ­ã‚°ã‚¤ãƒ³æˆåŠŸï¼")
        except Exception as e:
            print(f"ãƒ­ã‚°ã‚¤ãƒ³å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            raise

    def get(self, url, referer: Optional[str] = None, **kwargs):
        headers = build_headers(referer=referer)
        return self.session.get(url, headers=headers, **kwargs)

def parse_course_contents(html):
    soup = BeautifulSoup(html, 'html.parser')
    items = soup.find_all('section', class_='list-group-item cl-contentsList_listGroupItem')

    result = []
    for item in items:
        is_new = bool(item.find('div', class_='cl-contentsList_new'))
        title = ""; url = ""; share_link = ""

        if name_tag := item.find('h4', class_='cm-contentsList_contentName'):
            for new_tag in name_tag.find_all('div', class_='cl-contentsList_new'): new_tag.decompose()
            title = name_tag.get_text(strip=True)
            if a_tag := name_tag.find('a'):
                url = a_tag.get('href', '')
                if id_match := ID_REGEX.search(url):
                    share_link = f"{WEBCLASS_BASE_URL}/webclass/login.php?id={id_match.group(1)}&page=1&auth_mode=SAML"

        category = ""
        if category_tag := item.find('div', class_='cl-contentsList_categoryLabel'): category = category_tag.get_text(strip=True)

        period = ""
        for detail_item in item.find_all('div', class_='cm-contentsList_contentDetailListItem'):
            if label := detail_item.find('div', class_='cm-contentsList_contentDetailListItemLabel'):
                if "åˆ©ç”¨å¯èƒ½æœŸé–“" in label.get_text():
                    if data := detail_item.find('div', class_='cm-contentsList_contentDetailListItemData'): period = data.get_text(strip=True)
                    break

        result.append({
            'title': title, 'url': url, 'share_link': share_link, 'is_new': is_new,
            'category': category, 'period': period
        })
    return result

# --- main.py é–¢æ•° ---

def get_course_links(client: WebClassClient):
    print("ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã‹ã‚‰ç§‘ç›®ãƒªãƒ³ã‚¯ã‚’å–å¾—ä¸­...")
    try:
        top_html = client.get(client.dashboard_url).text
        soup = BeautifulSoup(top_html, "html.parser")
        course_links = set()
        for a_tag in soup.find_all('a', href=True, class_='list-group-item course'):
            raw_name = a_tag.get_text(strip=True)
            name = re.sub(r'^Â»?\s*\d+\s*', '', raw_name)
            href = a_tag.get("href")
            if href and '/webclass/course.php/' in href: course_links.add((name, href))
        print(f"ç§‘ç›®ãƒªãƒ³ã‚¯æŠ½å‡ºæ•°: {len(course_links)}")
        if not course_links: print("è­¦å‘Š: ç§‘ç›®ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ï¼")
        return list(course_links)
    except Exception as e:
        print(f"ç§‘ç›®ãƒªãƒ³ã‚¯ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        return []

def fetch_and_parse_course(course_info, client: WebClassClient):
    course_name, href = course_info
    session = client.session
    base_url = client.base_url

    try:
        url = urllib.parse.urljoin(base_url, href)
        res = session.get(url)
        html = res.text

        soup = BeautifulSoup(html, "html.parser")
        script = soup.find("script")
        if script and script.string and "window.location.href" in script.string:
            m = REDIRECT_REGEX.search(script.string)
            if m:
                redirect_url = urllib.parse.urljoin(base_url, m.group(1))
                res = session.get(redirect_url)
                html = res.text

        data = parse_course_contents(html)
        return course_name, data, "æˆåŠŸ"
    except Exception as e:
        return course_name, None, f"å¤±æ•—: {e}"

def send_data_to_gas(assignments):
    global GAS_WEB_APP_URL
    if not GAS_WEB_APP_URL or 'YOUR_PASTED_GAS_WEB_APP_URL_HERE' in GAS_WEB_APP_URL:
        print("ğŸš¨ GAS Webã‚¢ãƒ—ãƒªURLãŒæœªè¨­å®šã®ãŸã‚ã€ãƒ‡ãƒ¼ã‚¿é€ä¿¡ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã—ãŸã€‚")
        return

    try:
        headers = {'Content-Type': 'application/json'}
        payload = json.dumps({'assignments': assignments})

        print(f"GAS Webã‚¢ãƒ—ãƒªã«ãƒ‡ãƒ¼ã‚¿ã‚’é€ä¿¡ä¸­...")

        response = requests.post(GAS_WEB_APP_URL, headers=headers, data=payload)
        response.raise_for_status()

        gas_response = response.json()

        if gas_response.get('status') == 'SUCCESS':
            print("âœ… GASã¸ã®ãƒ‡ãƒ¼ã‚¿é€ä¿¡ã¨æ›¸ãè¾¼ã¿ã«æˆåŠŸã—ã¾ã—ãŸã€‚")
        else:
            print(f"ğŸš¨ GASå´ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {gas_response.get('message', 'ä¸æ˜ãªã‚¨ãƒ©ãƒ¼')}")

    except requests.exceptions.RequestException as e:
        print(f"ğŸš¨ GASã¸ã®HTTPé€šä¿¡ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
    except json.JSONDecodeError:
        print(f"ğŸš¨ GASã‹ã‚‰ã®å¿œç­”ãŒä¸æ­£ã§ã™: {response.text}")

def main():
    # 1. Colab Secretsã‹ã‚‰è³‡æ ¼æƒ…å ±å–å¾—
    try:
        from google.colab import userdata

        WEBCLASS_USERID = userdata.get('WEBCLASS_USERID')
        WEBCLASS_PASSWORD = userdata.get('WEBCLASS_PASSWORD')

        if not WEBCLASS_USERID or not WEBCLASS_PASSWORD:
            print("ğŸš¨ Colab Secretsã«èªè¨¼æƒ…å ±ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
            return

        userdata_dict = {"userid": WEBCLASS_USERID, "password": WEBCLASS_PASSWORD}
        print("WebClassèªè¨¼æƒ…å ±ã‚’Colab Secretsã‹ã‚‰å–å¾—ã—ã¾ã—ãŸã€‚")
    except ImportError:
        print("ğŸš¨ Colabç’°å¢ƒå¤–ã§å®Ÿè¡Œã•ã‚Œã¦ã„ã¾ã™ã€‚ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã¯Colabå°‚ç”¨ã§ã™ã€‚")
        return
    except Exception as e:
        print(f"èªè¨¼æƒ…å ±ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        return

    # 2. WebClassãƒ­ã‚°ã‚¤ãƒ³
    try:
        client = WebClassClient(userdata_dict["userid"], userdata_dict["password"])
    except Exception as e:
        print(f"ãƒ­ã‚°ã‚¤ãƒ³ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        return

    # 3. ç§‘ç›®ä¸€è¦§ã‚’å–å¾—
    course_links = get_course_links(client)
    if not course_links:
        print("å‡¦ç†ã™ã‚‹ç§‘ç›®ãŒã‚ã‚Šã¾ã›ã‚“ï¼çµ‚äº†ã—ã¾ã™ï¼")
        return

    # 4. å…¨ç§‘ç›®ã‚’å‡¦ç†ã—ã€èª²é¡Œãƒ‡ãƒ¼ã‚¿ã‚’åé›†
    all_assignments = []
    print(f"{len(course_links)}ä»¶ã®ç§‘ç›®ã‚’å‡¦ç†ã—ã¾ã™...")

    for i, course in enumerate(course_links):
        course_name, parsed_data, result = fetch_and_parse_course(course, client)
        print(f"  ({i+1}/{len(course_links)}) [{result}] - {course_name}")

        if result == "æˆåŠŸ" and parsed_data:
            for item in parsed_data:
                period_str = item.get('period', '')
                due_date_str = ''
                if ' - ' in period_str:
                    due_date_str = period_str.split(' - ')[1].strip()
                else:
                    due_date_str = period_str

                # GASã®åˆ—æ§‹æˆã«åˆã‚ã›ã¦ãƒ‡ãƒ¼ã‚¿ã‚’æ•´å½¢
                # [0:ã‚½ãƒ¼ã‚¹, 1:æˆæ¥­å, 2:èª²é¡Œã‚¿ã‚¤ãƒˆãƒ«, 3:ç· åˆ‡æ—¥æ™‚, 4:èª²é¡Œãƒªãƒ³ã‚¯ (URL), 5:Tasks ID, 6:ç™»éŒ²æ¸ˆã¿ãƒ•ãƒ©ã‚°]
                row = [
                    'WebClass',
                    course_name,
                    item['title'] + f" ({item['category']})",
                    due_date_str,
                    item['share_link'],
                    '',
                    ''
                ]
                all_assignments.append(row)

    if not all_assignments:
        print("WebClassã‹ã‚‰ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸï¼")
        return

    # 5. GAS Webã‚¢ãƒ—ãƒªã«ãƒ‡ãƒ¼ã‚¿ã‚’é€ä¿¡
    send_data_to_gas(all_assignments)

    print("ã™ã¹ã¦ã®å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸï¼")

# --- ãƒ¡ã‚¤ãƒ³é–¢æ•°ã®å‘¼ã³å‡ºã— ---
if __name__ == "__main__":
    main()